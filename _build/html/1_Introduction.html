
<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. Introduction to LangChain and Hugging Face &#8212; LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=109105fe" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css?v=6ad1a40c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=2b91c5f0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=ff8fa330"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=bcf87968"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1_Introduction';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="LLM" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.webp" class="logo__image only-light" alt="LLM - Home"/>
    <script>document.write(`<img src="_static/logo.webp" class="logo__image only-dark" alt="LLM - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    LLM
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Introduction to LangChain and Hugging Face</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/lucas-alves-souza/ml_notes/master?urlpath=tree/1_Introduction.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/lucas-alves-souza/ml_notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/lucas-alves-souza/ml_notes/issues/new?title=Issue%20on%20page%20%2F1_Introduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1_Introduction.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to LangChain and Hugging Face</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Introduction to LangChain and Hugging Face</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-and-settings">2. Packages and Settings</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#token">3. Token</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model">4. Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">4.1. Tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-pipeline">4.2. Creating the Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">4.2.1. Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-for-text-generation">4.3. Parameters for Text Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#max-new-tokens">4.3.1. <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#return-full-text">4.3.2. <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature">4.3.3. <code class="docutils literal notranslate"><span class="pre">temperature</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-sample">4.3.4. <code class="docutils literal notranslate"><span class="pre">do_sample</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-the-output-portuguese">4.4. Generating the Output (portuguese)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#templates-and-prompt-engineering">4.5. Templates and Prompt Engineering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-prompt-engineering">4.5.1. Exploring Prompt Engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-results">4.5.2. Improving Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#where-to-find-prompts">4.5.3. Where to Find Prompts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#message-format">4.6. Message Format</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#msg-input-messages">4.6.1. <code class="docutils literal notranslate"><span class="pre">msg</span></code>: Input Messages</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="introduction-to-langchain-and-hugging-face">
<h1><span class="section-number">1. </span>Introduction to LangChain and Hugging Face<a class="headerlink" href="#introduction-to-langchain-and-hugging-face" title="Permalink to this heading">#</a></h1>
</section>
<section id="packages-and-settings">
<h1><span class="section-number">2. </span>Packages and Settings<a class="headerlink" href="#packages-and-settings" title="Permalink to this heading">#</a></h1>
<p><em>Transformers</em> by HuggingFace offers a wide range of pre-trained models like BERT, GPT, and T5 for NLP tasks.</p>
<p><em>Einops</em> simplifies tensor manipulation with a clear syntax, making complex operations more straightforward.</p>
<p><em>Accelerate</em>, also by HuggingFace, helps optimize model training on various hardware accelerators such as GPUs and TPUs.</p>
<p><em>BitsAndBytes</em> enables efficient quantization of large models, reducing memory consumption in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !pip install -q transformers einops accelerate bitsandbytes</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="nn">Input In [2],</span> in <span class="ni">&lt;cell line: 1&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;transformers&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">getpass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cpu&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7a227c1a3d90&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="token">
<h1><span class="section-number">3. </span>Token<a class="headerlink" href="#token" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;HF_TOKEN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">getpass</span><span class="o">.</span><span class="n">getpass</span><span class="p">()</span>
<span class="c1"># in /home/lucas/Dropbox/Docs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>··········
</pre></div>
</div>
</div>
</div>
</section>
<section id="model">
<h1><span class="section-number">4. </span>Model<a class="headerlink" href="#model" title="Permalink to this heading">#</a></h1>
<p>Model from HuggingFace</p>
<p>Starting by showcasing Phi 3 (microsoft/Phi-3-mini-4k-instruct), a smaller model that has proven to be very interesting and comparable to much larger ones.</p>
<p>https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</p>
<p>open source, accessible, and performs well in Portuguese, although it still works better in English.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">id_model</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">id_model</span><span class="p">,</span>
    <span class="c1"># device_map = &quot;cuda&quot;,</span>
    <span class="n">torch_dtype</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">trust_remote_code</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;eager&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "116823487ce94ca3ae4ed9c21a0765df"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "025f39b0e30941b996dc846cc76c6156"}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:
- configuration_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "b96934b521ec4bc8acfa90de12a2e6dc"}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:
- modeling_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named &#39;flash_attn&#39;.
WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation=&#39;eager&#39;`.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "4308ebe9731f4e13b3a8bee53cb93c20"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "2b020b4067f840bb8d2a670272d45d0a"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "391b789071624f39bbc9ef3a7292f578"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "3e4aa370dceb4f999eddf9a73180da0e"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "588196b8efde488cadfce1b12f4b5d4c"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "56226b327aa945bda2725b4d84e7f93d"}</script></div>
</div>
<p><em>device_map=”cuda”:</em> Specifies that the model should be loaded onto a CUDA-enabled GPU. GPUs significantly improve inference and training performance by leveraging parallel processing.</p>
<p><em>torch_dtype=”auto”:</em> Automatically sets the appropriate data type for the model’s tensors. This ensures the model uses the best data type for performance and memory efficiency, typically float32 or float16.</p>
<p><em>trust_remote_code=True:</em> Allows the loading of custom code from the model repository on HuggingFace. This is necessary for certain models that require specific configurations or implementations not included in the standard library.</p>
<p><em>attn_implementation=”eager”:</em> Specifies the implementation method for the attention mechanism. The “eager” setting is a particular implementation that may offer better performance for some models by processing the attention mechanism in a specific way.</p>
<section id="tokenizer">
<h2><span class="section-number">4.1. </span>Tokenizer<a class="headerlink" href="#tokenizer" title="Permalink to this heading">#</a></h2>
<p>Ee also need to load the tokenizer associated with the model. The tokenizer is essential for preparing text data into a format that the model can understand.</p>
<p>A tokenizer converts raw text into tokens, which are numerical representations that the model can process. It also converts the model’s output tokens back into human-readable text.
Tokenizers handle tasks such as splitting text into words or subwords, adding special tokens, and managing vocabulary mapping.
[more details in the slides]</p>
<p>The tokenizer is a critical component in the NLP pipeline, bridging the gap between raw text and model-ready tokens.</p>
<p>To implement this, we will use the AutoTokenizer.from_pretrained() function, specifying the same tokenizer as the model. This ensures consistency in text processing during both training and inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">id_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "b3378a8d6ddb4dfaac57fcece2d41146"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "1d387edc66524181a26a0fba13263eb9"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "27b625af099242de875c558b58823483"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "291b747f781345b183130a4519ad2c96"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "f2291938e0164dc794be2692541df3e9"}</script></div>
</div>
</section>
<section id="creating-the-pipeline">
<h2><span class="section-number">4.2. </span>Creating the Pipeline<a class="headerlink" href="#creating-the-pipeline" title="Permalink to this heading">#</a></h2>
<p>Now we will create a pipeline for text generation using the model and tokenizer we loaded earlier. HuggingFace’s pipeline function simplifies the process of executing various natural language processing tasks by providing a high-level interface.</p>
<p>A pipeline is an abstraction that simplifies the use of pre-trained models for a variety of NLP tasks. It provides a unified API for different tasks, such as text generation, text classification, translation, and more.</p>
<blockquote>
<div><p>[More details in the slides]</p>
</div></blockquote>
<section id="parameters">
<h3><span class="section-number">4.2.1. </span>Parameters:<a class="headerlink" href="#parameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">&quot;text-generation&quot;</span></code></strong>: Specifies the task the pipeline is set up to perform. In this case, we are configuring a pipeline for text generation. The pipeline will use the model to generate text based on a given prompt.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">model=model</span></code></strong>: Specifies the pre-trained model the pipeline will use. Here, we are passing the previously loaded model. This model is responsible for generating text based on the input tokens.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">tokenizer=tokenizer</span></code></strong>: Specifies the tokenizer the pipeline will use. We pass the previously loaded tokenizer to ensure that the input text is correctly tokenized and the output tokens are accurately decoded.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Device set to use cpu
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="parameters-for-text-generation">
<h2><span class="section-number">4.3. </span>Parameters for Text Generation<a class="headerlink" href="#parameters-for-text-generation" title="Permalink to this heading">#</a></h2>
<p>To customize the behavior of our text generation pipeline, we can pass a dictionary of arguments to control various aspects of the generation process.</p>
<section id="max-new-tokens">
<h3><span class="section-number">4.3.1. </span><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code><a class="headerlink" href="#max-new-tokens" title="Permalink to this heading">#</a></h3>
<p>This parameter specifies the maximum number of new tokens to be generated in response to the input prompt. It controls the length of the generated text.</p>
<ul class="simple">
<li><p><strong>Example</strong>: Setting <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code> to 500 means the model will generate up to 500 tokens beyond the input prompt.</p></li>
</ul>
</section>
<section id="return-full-text">
<h3><span class="section-number">4.3.2. </span><code class="docutils literal notranslate"><span class="pre">return_full_text</span></code><a class="headerlink" href="#return-full-text" title="Permalink to this heading">#</a></h3>
<p>Determines whether to return the full text, including the input prompt, or only the newly generated tokens.</p>
<ul class="simple">
<li><p><strong>Example</strong>: Setting <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> means only the newly generated tokens will be returned, excluding the original input prompt. If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned text will include both the input prompt and the generated continuation.</p></li>
</ul>
</section>
<section id="temperature">
<h3><span class="section-number">4.3.3. </span><code class="docutils literal notranslate"><span class="pre">temperature</span></code><a class="headerlink" href="#temperature" title="Permalink to this heading">#</a></h3>
<p>Controls the randomness of the text generation process. Lower values make the model’s output more deterministic and focused, while higher values increase randomness and creativity.</p>
<ul class="simple">
<li><p><strong>Example</strong>: A <code class="docutils literal notranslate"><span class="pre">temperature</span></code> of <code class="docutils literal notranslate"><span class="pre">0.1</span></code> makes the model’s predictions more reliable and less varied, leading to more predictable outputs. A higher <code class="docutils literal notranslate"><span class="pre">temperature</span></code> would result in more diverse and varied text.</p></li>
</ul>
</section>
<section id="do-sample">
<h3><span class="section-number">4.3.4. </span><code class="docutils literal notranslate"><span class="pre">do_sample</span></code><a class="headerlink" href="#do-sample" title="Permalink to this heading">#</a></h3>
<p>This parameter enables or disables sampling during text generation. When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the model samples tokens based on their probabilities, adding an element of randomness to the output. When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the model always selects the token with the highest probability (greedy decoding).</p>
<ul class="simple">
<li><p><strong>Example</strong>: Setting <code class="docutils literal notranslate"><span class="pre">do_sample</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> allows for more diverse and creative text generation. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the output will be more deterministic but potentially less engaging.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>  <span class="c1"># ou 50 para testar com um número menor de tokens</span>
    <span class="s2">&quot;return_full_text&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="generating-the-output-portuguese">
<h2><span class="section-number">4.4. </span>Generating the Output (portuguese)<a class="headerlink" href="#generating-the-output-portuguese" title="Permalink to this heading">#</a></h2>
<p>The following line of code passes the input message and generation arguments to the text generation pipeline:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
</pre></div>
</div>
<p>**generation_args: This unpacks the generation_args dictionary and passes its contents as keyword arguments to the pipeline, customizing the text generation process. This allows fine-tuning of the generation behavior by adjusting parameters such as max_new_tokens, temperature, and more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explique o que é computação quântica&quot;</span>
<span class="c1">#prompt = &quot;Quanto é 7 x 6 - 42?&quot;</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48
WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;generated_text&#39;: &#39; e como ela difere da computação clássica.\n\n\n### Solution:\n\nA computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits binários (0 ou 1) para representar dados, a computação quântica utiliza qubits, que pode&#39;}]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> e como ela difere da computação clássica.


### Solution:

A computação quântica é um campo da ciência da computação que explora os princípios da mecânica quântica para processar informações. Ao contrário da computação clássica, que usa bits binários (0 ou 1) para representar dados, a computação quântica utiliza qubits, que pode
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Quem foi a primeira pessoa no espaço?&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-15-1db7a4ba7888&gt;</span> in <span class="ni">&lt;cell line: 2&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Quem foi a primeira pessoa no espaço?&quot;</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py</span> in <span class="ni">__call__</span><span class="nt">(self, text_inputs, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">270</span>                 <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">chats</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">271</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">272</span>             <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">text_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">273</span> 
<span class="g g-Whitespace">    </span><span class="mi">274</span>     <span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py</span> in <span class="ni">__call__</span><span class="nt">(self, inputs, num_workers, batch_size, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1299</span>             <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1300</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1301</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_single</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">preprocess_params</span><span class="p">,</span> <span class="n">forward_params</span><span class="p">,</span> <span class="n">postprocess_params</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1302</span> 
<span class="g g-Whitespace">   </span><span class="mi">1303</span>     <span class="k">def</span><span class="w"> </span><span class="nf">run_multi</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">preprocess_params</span><span class="p">,</span> <span class="n">forward_params</span><span class="p">,</span> <span class="n">postprocess_params</span><span class="p">):</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py</span> in <span class="ni">run_single</span><span class="nt">(self, inputs, preprocess_params, forward_params, postprocess_params)</span>
<span class="g g-Whitespace">   </span><span class="mi">1306</span>     <span class="k">def</span><span class="w"> </span><span class="nf">run_single</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">preprocess_params</span><span class="p">,</span> <span class="n">forward_params</span><span class="p">,</span> <span class="n">postprocess_params</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1307</span>         <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">preprocess_params</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1308</span>         <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">forward_params</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1309</span>         <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="o">**</span><span class="n">postprocess_params</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1310</span>         <span class="k">return</span> <span class="n">outputs</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py</span> in <span class="ni">forward</span><span class="nt">(self, model_inputs, **forward_params)</span>
<span class="g g-Whitespace">   </span><span class="mi">1206</span>                 <span class="k">with</span> <span class="n">inference_context</span><span class="p">():</span>
<span class="g g-Whitespace">   </span><span class="mi">1207</span>                     <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_tensor_on_device</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1208</span>                     <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">forward_params</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1209</span>                     <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_tensor_on_device</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1210</span>             <span class="k">else</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py</span> in <span class="ni">_forward</span><span class="nt">(self, model_inputs, **generate_kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">368</span>             <span class="n">generate_kwargs</span><span class="p">[</span><span class="s2">&quot;generation_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span>
<span class="g g-Whitespace">    </span><span class="mi">369</span> 
<span class="ne">--&gt; </span><span class="mi">370</span>         <span class="n">generated_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">371</span>         <span class="n">out_b</span> <span class="o">=</span> <span class="n">generated_sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">372</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py</span> in <span class="ni">decorate_context</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">114</span>     <span class="k">def</span><span class="w"> </span><span class="nf">decorate_context</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">115</span>         <span class="k">with</span> <span class="n">ctx_factory</span><span class="p">():</span>
<span class="ne">--&gt; </span><span class="mi">116</span>             <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">117</span> 
<span class="g g-Whitespace">    </span><span class="mi">118</span>     <span class="k">return</span> <span class="n">decorate_context</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py</span> in <span class="ni">generate</span><span class="nt">(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2250</span> 
<span class="g g-Whitespace">   </span><span class="mi">2251</span>             <span class="c1"># 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)</span>
<span class="ne">-&gt; </span><span class="mi">2252</span>             <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2253</span>                 <span class="n">input_ids</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2254</span>                 <span class="n">logits_processor</span><span class="o">=</span><span class="n">prepared_logits_processor</span><span class="p">,</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py</span> in <span class="ni">_sample</span><span class="nt">(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">3249</span> 
<span class="g g-Whitespace">   </span><span class="mi">3250</span>             <span class="k">if</span> <span class="n">is_prefill</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">3251</span>                 <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">3252</span>                 <span class="n">is_prefill</span> <span class="o">=</span> <span class="kc">False</span>
<span class="g g-Whitespace">   </span><span class="mi">3253</span>             <span class="k">else</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py</span> in <span class="ni">forward</span><span class="nt">(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)</span>
<span class="g g-Whitespace">   </span><span class="mi">1241</span> 
<span class="g g-Whitespace">   </span><span class="mi">1242</span>         <span class="c1"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>
<span class="ne">-&gt; </span><span class="mi">1243</span>         <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1244</span>             <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1245</span>             <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py</span> in <span class="ni">forward</span><span class="nt">(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)</span>
<span class="g g-Whitespace">   </span><span class="mi">1119</span>                 <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1120</span>             <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1121</span>                 <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">decoder_layer</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1122</span>                     <span class="n">hidden_states</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1123</span>                     <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py</span> in <span class="ni">forward</span><span class="nt">(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">840</span> 
<span class="g g-Whitespace">    </span><span class="mi">841</span>         <span class="c1"># Self Attention</span>
<span class="ne">--&gt; </span><span class="mi">842</span>         <span class="n">attn_outputs</span><span class="p">,</span> <span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">present_key_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">843</span>             <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">844</span>             <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py</span> in <span class="ni">forward</span><span class="nt">(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)</span>
<span class="g g-Whitespace">    </span><span class="mi">313</span>         <span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">314</span> 
<span class="ne">--&gt; </span><span class="mi">315</span>         <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">316</span>         <span class="n">query_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span>
<span class="g g-Whitespace">    </span><span class="mi">317</span>         <span class="n">query_states</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">query_pos</span><span class="p">]</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">123</span> 
<span class="g g-Whitespace">    </span><span class="mi">124</span>     <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">125</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> 
<span class="g g-Whitespace">    </span><span class="mi">127</span>     <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="templates-and-prompt-engineering">
<h2><span class="section-number">4.5. </span>Templates and Prompt Engineering<a class="headerlink" href="#templates-and-prompt-engineering" title="Permalink to this heading">#</a></h2>
<p>Prompt templates help translate the user’s input and parameters into instructions for a language model. This can be used to guide the model’s response, helping it understand the context and generate relevant and more coherent output.</p>
<blockquote>
<div><p><strong>Solving the problem of text continuing after the response</strong></p>
</div></blockquote>
<p>To discover the appropriate template, always check the model description, for example: <a class="reference external" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct">https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</a>.</p>
<p>For Phi 3, the authors recommend the following template.</p>
<p>Note: Later, we will see a way to retrieve this template manually without having to copy and paste it here.</p>
<p>These tags formed by <code class="docutils literal notranslate"><span class="pre">&lt;|##name##|&gt;</span></code> are what we call <strong>special tokens</strong> and are used to delimit the beginning and end of text, telling the model how we want the message to be interpreted.</p>
<p>The special tokens used to interact with Phi 3 are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;|system|&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;|user|&gt;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&lt;|assistant|&gt;</span></code>: correspond to the roles of the messages. The roles used here are: system, user, and assistant.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;|end|&gt;</span></code>: This is equivalent to the EOS (End of String) token, used to mark the end of the text/string.</p></li>
</ul>
<p>We will use <code class="docutils literal notranslate"><span class="pre">.format</span></code> to concatenate the prompt into this template so we don’t have to manually rewrite it every time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|system|&gt;</span>
<span class="s2">You are a helpful assistant.&lt;|end|&gt;</span>
<span class="s2">&lt;|user|&gt;</span>
<span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;&lt;|end|&gt;</span>
<span class="s2">&lt;|assistant|&gt;&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">template</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> A primeira pessoa a ir ao espaço foi Yuri Gagarin, um cosmonauta soviético. Ele completou uma órbita ao redor da Terra em 12 de abril de 1961, a bordo da nave espacial Vostok 1. Sua missão marcou um momento histórico na exploração espacial e foi um grande passo para a humanidade na conquista do espaço.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Você entende português?&quot;</span>

<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|system|&gt;</span>
<span class="s2">You are a helpful assistant.&lt;|end|&gt;</span>
<span class="s2">&lt;|user|&gt;</span>
<span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;&lt;|end|&gt;</span>
<span class="s2">&lt;|assistant|&gt;&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">template</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;O que é IA?&quot;</span>  <span class="c1"># @param {type:&quot;string&quot;}</span>

<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|system|&gt;</span>
<span class="s2">You are a helpful assistant.&lt;|end|&gt;</span>
<span class="s2">&lt;|user|&gt;</span>
<span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;&lt;|end|&gt;</span>
<span class="s2">&lt;|assistant|&gt;&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">template</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<section id="exploring-prompt-engineering">
<h3><span class="section-number">4.5.1. </span>Exploring Prompt Engineering<a class="headerlink" href="#exploring-prompt-engineering" title="Permalink to this heading">#</a></h3>
<p>In addition to slightly modifying the system prompt to make the result more suitable:</p>
<ul class="simple">
<li><p>For example, we can add “Answer in 1 sentence” after our question (“What is AI?”).</p></li>
<li><p>Another example: “Answer in the form of a poem.”</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#prompt = &quot;O que é IA? &quot;  # @param {type:&quot;string&quot;}</span>
<span class="c1">#prompt = &quot;O que é IA? Responda em 1 frase&quot; # @param {type:&quot;string&quot;}</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;O que é IA? Responda em forma de poema&quot;</span> <span class="c1"># @param {type:&quot;string&quot;}</span>

<span class="n">sys_prompt</span> <span class="o">=</span> <span class="s2">&quot;Você é um assistente virtual prestativo. Responda as perguntas em português.&quot;</span>

<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|system|&gt;</span>
<span class="si">{}</span><span class="s2">&lt;|end|&gt;</span>
<span class="s2">&lt;|user|&gt;</span>
<span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;&lt;|end|&gt;</span>
<span class="s2">&lt;|assistant|&gt;&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sys_prompt</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">template</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Gere um código em python que escreva a sequência de fibonnaci&quot;</span>

<span class="n">sys_prompt</span> <span class="o">=</span> <span class="s2">&quot;Você é um programador experiente. Retorne o código requisitado e forneça explicações breves se achar conveniente&quot;</span>

<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|system|&gt;</span>
<span class="si">{}</span><span class="s2">&lt;|end|&gt;</span>
<span class="s2">&lt;|user|&gt;</span>
<span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;&lt;|end|&gt;</span>
<span class="s2">&lt;|assistant|&gt;&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sys_prompt</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">template</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">  File</span><span class="nn"> &quot;&lt;ipython-input-19-81eee6fc6769&gt;&quot;</span><span class="gt">, line </span><span class="mi">3</span>
    <span class="n">In</span> <span class="n">addition</span> <span class="n">to</span> <span class="n">slightly</span> <span class="n">modifying</span> <span class="n">the</span> <span class="n">system</span> <span class="n">prompt</span> <span class="n">to</span> <span class="n">make</span> <span class="n">the</span> <span class="n">result</span> <span class="n">more</span> <span class="n">suitable</span><span class="p">:</span>
       <span class="o">^</span>
<span class="ne">SyntaxError</span>: invalid syntax
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>

    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">sequence</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>

        <span class="n">sequence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">sequence</span>


<span class="c1"># Exemplo de uso:</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Quantidade de números da sequência de Fibonacci a serem gerados</span>

<span class="nb">print</span><span class="p">(</span><span class="n">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="improving-results">
<h3><span class="section-number">4.5.2. </span>Improving Results<a class="headerlink" href="#improving-results" title="Permalink to this heading">#</a></h3>
<p><strong>Exploring Prompt Changes</strong></p>
<ul>
<li><p>It may fail depending on the type of request. Various ways to improve the output will be explored.</p></li>
<li><p>For now, remember to first check if your prompt could be more specific. If even after improving the prompt you are struggling to achieve the expected result (and after experimenting with other parameters), the model may not be suitable for this task.</p>
<ul>
<li><blockquote>
<div><p>Bonus tip for code generation: A suggestion for a convenient prompt when using an LLM as your co-pilot:<br />
“Refactor using concepts like SOLID, Clean Code, DRY, KISS, and if possible, apply one or more appropriate design patterns aiming for scalability and performance, creating an organized folder structure and separating files accordingly.”<br />
(Of course, feel free to modify this as needed.)</p>
</div></blockquote>
</li>
</ul>
</li>
<li><p>Note: Sometimes, keeping the prompt simple and not overly elaborate works better. Adding too much or including unrelated references can “confuse” the model. Therefore, it’s often useful to add or remove terms incrementally when experimenting to achieve better results.</p></li>
</ul>
<p><strong>Exploring Other Models</strong></p>
<ul class="simple">
<li><p>In this case, to achieve more accurate results, you could look for larger and more modern models with more parameters (keeping in mind the trade-off between efficiency and response quality) or models specialized in the desired task, such as code generation or conversations/chat.</p>
<ul>
<li><p>For example, for code generation, you could use the model <a class="reference external" href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct">deepseek-coder 6.7B</a> (or search for others focused on this area).</p></li>
</ul>
</li>
</ul>
</section>
<section id="where-to-find-prompts">
<h3><span class="section-number">4.5.3. </span>Where to Find Prompts<a class="headerlink" href="#where-to-find-prompts" title="Permalink to this heading">#</a></h3>
<p>Creating your own prompt can be ideal if you’re aiming for very specific cases.<br />
However, if you’re short on time to experiment (or unsure of the best approach), a good tip is to search for prompts online.</p>
<p>There are many websites and repositories where the community shares ready-made prompts.</p>
<p>One example is the LangSmith hub: <a class="reference external" href="https://smith.langchain.com/hub">https://smith.langchain.com/hub</a>.<br />
It is part of the LangChain ecosystem. This will be very convenient later, as we’ll see how to fetch prompts hosted there using just a single function.</p>
</section>
</section>
<section id="message-format">
<h2><span class="section-number">4.6. </span>Message Format<a class="headerlink" href="#message-format" title="Permalink to this heading">#</a></h2>
<p>A growing use case for LLMs is chat. In a chat context, instead of continuing a single text sequence (as with a standard language model), the model continues a conversation consisting of one or more messages. Each message includes a role, such as “user” or “assistant,” along with the message text.</p>
<p>The prompt can therefore be structured as shown below. We’ll explore this in more detail when using LangChain, as it provides additional resources to enhance this mode.</p>
<section id="msg-input-messages">
<h3><span class="section-number">4.6.1. </span><code class="docutils literal notranslate"><span class="pre">msg</span></code>: Input Messages<a class="headerlink" href="#msg-input-messages" title="Permalink to this heading">#</a></h3>
<p>This list contains the input message to which we want the model to respond. Each message is a dictionary with the following keys:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">role</span></code></strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;user&quot;</span></code> indicates that the message comes from the user.</p></li>
<li><p>Other roles may include <code class="docutils literal notranslate"><span class="pre">&quot;system&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;assistant&quot;</span></code> if you’re simulating a multi-turn conversation. Different models may use different role names. For Phi 3, these roles are expected.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">content</span></code></strong>:</p>
<ul>
<li><p>This contains the actual query or prompt you want the model to respond to.</p></li>
</ul>
</li>
</ul>
<p>We’ll delve deeper into this mode when we start working with LangChain.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LLM</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Introduction to LangChain and Hugging Face</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-and-settings">2. Packages and Settings</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#token">3. Token</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model">4. Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">4.1. Tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-pipeline">4.2. Creating the Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">4.2.1. Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-for-text-generation">4.3. Parameters for Text Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#max-new-tokens">4.3.1. <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#return-full-text">4.3.2. <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature">4.3.3. <code class="docutils literal notranslate"><span class="pre">temperature</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-sample">4.3.4. <code class="docutils literal notranslate"><span class="pre">do_sample</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-the-output-portuguese">4.4. Generating the Output (portuguese)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#templates-and-prompt-engineering">4.5. Templates and Prompt Engineering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-prompt-engineering">4.5.1. Exploring Prompt Engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-results">4.5.2. Improving Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#where-to-find-prompts">4.5.3. Where to Find Prompts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#message-format">4.6. Message Format</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#msg-input-messages">4.6.1. <code class="docutils literal notranslate"><span class="pre">msg</span></code>: Input Messages</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lucas A. Souza
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>