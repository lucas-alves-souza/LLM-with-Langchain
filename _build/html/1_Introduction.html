
<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. Introduction to LangChain and Hugging Face &#8212; LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=109105fe" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css?v=6ad1a40c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=2b91c5f0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=ff8fa330"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=bcf87968"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1_Introduction';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="LLM" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.webp" class="logo__image only-light" alt="LLM - Home"/>
    <script>document.write(`<img src="_static/logo.webp" class="logo__image only-dark" alt="LLM - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    LLM
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Introduction to LangChain and Hugging Face</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/lucas-alves-souza/LLM-with-Langchain/master?urlpath=tree/1_Introduction.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/lucas-alves-souza/LLM-with-Langchain" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/lucas-alves-souza/LLM-with-Langchain/issues/new?title=Issue%20on%20page%20%2F1_Introduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1_Introduction.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to LangChain and Hugging Face</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Introduction to LangChain and Hugging Face</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-and-settings">2. Packages and Settings</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#token">3. Token</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model">4. Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">4.1. Tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-pipeline">4.2. Creating the Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">4.2.1. Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-for-text-generation">4.3. Parameters for Text Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#max-new-tokens">4.3.1. <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#return-full-text">4.3.2. <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature">4.3.3. <code class="docutils literal notranslate"><span class="pre">temperature</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-sample">4.3.4. <code class="docutils literal notranslate"><span class="pre">do_sample</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-the-output-portuguese">4.4. Generating the Output (portuguese)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#templates-and-prompt-engineering">4.5. Templates and Prompt Engineering</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="introduction-to-langchain-and-hugging-face">
<h1><span class="section-number">1. </span>Introduction to LangChain and Hugging Face<a class="headerlink" href="#introduction-to-langchain-and-hugging-face" title="Permalink to this heading">#</a></h1>
</section>
<section id="packages-and-settings">
<h1><span class="section-number">2. </span>Packages and Settings<a class="headerlink" href="#packages-and-settings" title="Permalink to this heading">#</a></h1>
<p><em>Transformers</em> by HuggingFace offers a wide range of pre-trained models like BERT, GPT, and T5 for NLP tasks.</p>
<p><em>Einops</em> simplifies tensor manipulation with a clear syntax, making complex operations more straightforward.</p>
<p><em>Accelerate</em>, also by HuggingFace, helps optimize model training on various hardware accelerators such as GPUs and TPUs.</p>
<p><em>BitsAndBytes</em> enables efficient quantization of large models, reducing memory consumption in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !pip install -q transformers einops accelerate bitsandbytes</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="nn">Input In [2],</span> in <span class="ni">&lt;cell line: 1&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;transformers&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">getpass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cpu&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7a227c1a3d90&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="token">
<h1><span class="section-number">3. </span>Token<a class="headerlink" href="#token" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;HF_TOKEN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">getpass</span><span class="o">.</span><span class="n">getpass</span><span class="p">()</span>
<span class="c1"># in /home/lucas/Dropbox/Docs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>··········
</pre></div>
</div>
</div>
</div>
</section>
<section id="model">
<h1><span class="section-number">4. </span>Model<a class="headerlink" href="#model" title="Permalink to this heading">#</a></h1>
<p>Model from HuggingFace</p>
<p>Starting by showcasing Phi 3 (microsoft/Phi-3-mini-4k-instruct), a smaller model that has proven to be very interesting and comparable to much larger ones.</p>
<p>https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</p>
<p>open source, accessible, and performs well in Portuguese, although it still works better in English.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">id_model</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">id_model</span><span class="p">,</span>
    <span class="c1"># device_map = &quot;cuda&quot;,</span>
    <span class="n">torch_dtype</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">trust_remote_code</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;eager&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "116823487ce94ca3ae4ed9c21a0765df"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "025f39b0e30941b996dc846cc76c6156"}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:
- configuration_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "b96934b521ec4bc8acfa90de12a2e6dc"}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:
- modeling_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named &#39;flash_attn&#39;.
WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation=&#39;eager&#39;`.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "4308ebe9731f4e13b3a8bee53cb93c20"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "2b020b4067f840bb8d2a670272d45d0a"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "391b789071624f39bbc9ef3a7292f578"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "3e4aa370dceb4f999eddf9a73180da0e"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "588196b8efde488cadfce1b12f4b5d4c"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "56226b327aa945bda2725b4d84e7f93d"}</script></div>
</div>
<p><em>device_map=”cuda”:</em> Specifies that the model should be loaded onto a CUDA-enabled GPU. GPUs significantly improve inference and training performance by leveraging parallel processing.</p>
<p><em>torch_dtype=”auto”:</em> Automatically sets the appropriate data type for the model’s tensors. This ensures the model uses the best data type for performance and memory efficiency, typically float32 or float16.</p>
<p><em>trust_remote_code=True:</em> Allows the loading of custom code from the model repository on HuggingFace. This is necessary for certain models that require specific configurations or implementations not included in the standard library.</p>
<p><em>attn_implementation=”eager”:</em> Specifies the implementation method for the attention mechanism. The “eager” setting is a particular implementation that may offer better performance for some models by processing the attention mechanism in a specific way.</p>
<section id="tokenizer">
<h2><span class="section-number">4.1. </span>Tokenizer<a class="headerlink" href="#tokenizer" title="Permalink to this heading">#</a></h2>
<p>Ee also need to load the tokenizer associated with the model. The tokenizer is essential for preparing text data into a format that the model can understand.</p>
<p>A tokenizer converts raw text into tokens, which are numerical representations that the model can process. It also converts the model’s output tokens back into human-readable text.
Tokenizers handle tasks such as splitting text into words or subwords, adding special tokens, and managing vocabulary mapping.
[more details in the slides]</p>
<p>The tokenizer is a critical component in the NLP pipeline, bridging the gap between raw text and model-ready tokens.</p>
<p>To implement this, we will use the AutoTokenizer.from_pretrained() function, specifying the same tokenizer as the model. This ensures consistency in text processing during both training and inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">id_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "b3378a8d6ddb4dfaac57fcece2d41146"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "1d387edc66524181a26a0fba13263eb9"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "27b625af099242de875c558b58823483"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "291b747f781345b183130a4519ad2c96"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "f2291938e0164dc794be2692541df3e9"}</script></div>
</div>
</section>
<section id="creating-the-pipeline">
<h2><span class="section-number">4.2. </span>Creating the Pipeline<a class="headerlink" href="#creating-the-pipeline" title="Permalink to this heading">#</a></h2>
<p>Now we will create a pipeline for text generation using the model and tokenizer we loaded earlier. HuggingFace’s pipeline function simplifies the process of executing various natural language processing tasks by providing a high-level interface.</p>
<p>A pipeline is an abstraction that simplifies the use of pre-trained models for a variety of NLP tasks. It provides a unified API for different tasks, such as text generation, text classification, translation, and more.</p>
<blockquote>
<div><p>[More details in the slides]</p>
</div></blockquote>
<section id="parameters">
<h3><span class="section-number">4.2.1. </span>Parameters:<a class="headerlink" href="#parameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">&quot;text-generation&quot;</span></code></strong>: Specifies the task the pipeline is set up to perform. In this case, we are configuring a pipeline for text generation. The pipeline will use the model to generate text based on a given prompt.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">model=model</span></code></strong>: Specifies the pre-trained model the pipeline will use. Here, we are passing the previously loaded model. This model is responsible for generating text based on the input tokens.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">tokenizer=tokenizer</span></code></strong>: Specifies the tokenizer the pipeline will use. We pass the previously loaded tokenizer to ensure that the input text is correctly tokenized and the output tokens are accurately decoded.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Device set to use cpu
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="parameters-for-text-generation">
<h2><span class="section-number">4.3. </span>Parameters for Text Generation<a class="headerlink" href="#parameters-for-text-generation" title="Permalink to this heading">#</a></h2>
<p>To customize the behavior of our text generation pipeline, we can pass a dictionary of arguments to control various aspects of the generation process.</p>
<section id="max-new-tokens">
<h3><span class="section-number">4.3.1. </span><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code><a class="headerlink" href="#max-new-tokens" title="Permalink to this heading">#</a></h3>
<p>This parameter specifies the maximum number of new tokens to be generated in response to the input prompt. It controls the length of the generated text.</p>
<ul class="simple">
<li><p><strong>Example</strong>: Setting <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code> to 500 means the model will generate up to 500 tokens beyond the input prompt.</p></li>
</ul>
</section>
<section id="return-full-text">
<h3><span class="section-number">4.3.2. </span><code class="docutils literal notranslate"><span class="pre">return_full_text</span></code><a class="headerlink" href="#return-full-text" title="Permalink to this heading">#</a></h3>
<p>Determines whether to return the full text, including the input prompt, or only the newly generated tokens.</p>
<ul class="simple">
<li><p><strong>Example</strong>: Setting <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> means only the newly generated tokens will be returned, excluding the original input prompt. If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned text will include both the input prompt and the generated continuation.</p></li>
</ul>
</section>
<section id="temperature">
<h3><span class="section-number">4.3.3. </span><code class="docutils literal notranslate"><span class="pre">temperature</span></code><a class="headerlink" href="#temperature" title="Permalink to this heading">#</a></h3>
<p>Controls the randomness of the text generation process. Lower values make the model’s output more deterministic and focused, while higher values increase randomness and creativity.</p>
<ul class="simple">
<li><p><strong>Example</strong>: A <code class="docutils literal notranslate"><span class="pre">temperature</span></code> of <code class="docutils literal notranslate"><span class="pre">0.1</span></code> makes the model’s predictions more reliable and less varied, leading to more predictable outputs. A higher <code class="docutils literal notranslate"><span class="pre">temperature</span></code> would result in more diverse and varied text.</p></li>
</ul>
</section>
<section id="do-sample">
<h3><span class="section-number">4.3.4. </span><code class="docutils literal notranslate"><span class="pre">do_sample</span></code><a class="headerlink" href="#do-sample" title="Permalink to this heading">#</a></h3>
<p>This parameter enables or disables sampling during text generation. When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the model samples tokens based on their probabilities, adding an element of randomness to the output. When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the model always selects the token with the highest probability (greedy decoding).</p>
<ul class="simple">
<li><p><strong>Example</strong>: Setting <code class="docutils literal notranslate"><span class="pre">do_sample</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> allows for more diverse and creative text generation. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the output will be more deterministic but potentially less engaging.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>  <span class="c1"># ou 50 para testar com um número menor de tokens</span>
    <span class="s2">&quot;return_full_text&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="generating-the-output-portuguese">
<h2><span class="section-number">4.4. </span>Generating the Output (portuguese)<a class="headerlink" href="#generating-the-output-portuguese" title="Permalink to this heading">#</a></h2>
<p>The following line of code passes the input message and generation arguments to the text generation pipeline:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
</pre></div>
</div>
<p>**generation_args: This unpacks the generation_args dictionary and passes its contents as keyword arguments to the pipeline, customizing the text generation process. This allows fine-tuning of the generation behavior by adjusting parameters such as max_new_tokens, temperature, and more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explique o que é computação quântica&quot;</span>
<span class="c1">#prompt = &quot;Quanto é 7 x 6 - 42?&quot;</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48
WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Quem foi a primeira pessoa no espaço?&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="templates-and-prompt-engineering">
<h2><span class="section-number">4.5. </span>Templates and Prompt Engineering<a class="headerlink" href="#templates-and-prompt-engineering" title="Permalink to this heading">#</a></h2>
<p>Prompt templates help translate the user’s input and parameters into instructions for a language model. This can be used to guide the model’s response, helping it understand the context and generate relevant and more coherent output.</p>
<blockquote>
<div><p><strong>Solving the problem of text continuing after the response</strong></p>
</div></blockquote>
<p>To discover the appropriate template, always check the model description, for example: <a class="reference external" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct">https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</a>.</p>
<p>For Phi 3, the authors recommend the following template.</p>
<p>Note: Later, we will see a way to retrieve this template manually without having to copy and paste it here.</p>
<p>These tags formed by <code class="docutils literal notranslate"><span class="pre">&lt;|##name##|&gt;</span></code> are what we call <strong>special tokens</strong> and are used to delimit the beginning and end of text, telling the model how we want the message to be interpreted.</p>
<p>The special tokens used to interact with Phi 3 are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;|system|&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;|user|&gt;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&lt;|assistant|&gt;</span></code>: correspond to the roles of the messages. The roles used here are: system, user, and assistant.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;|end|&gt;</span></code>: This is equivalent to the EOS (End of String) token, used to mark the end of the text/string.</p></li>
</ul>
<p>We will use <code class="docutils literal notranslate"><span class="pre">.format</span></code> to concatenate the prompt into this template so we don’t have to manually rewrite it every time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|system|&gt;</span>
<span class="s2">You are a helpful assistant.&lt;|end|&gt;</span>
<span class="s2">&lt;|user|&gt;</span>
<span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;&lt;|end|&gt;</span>
<span class="s2">&lt;|assistant|&gt;&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">template</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LLM</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Introduction to LangChain and Hugging Face</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-and-settings">2. Packages and Settings</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#token">3. Token</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model">4. Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizer">4.1. Tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-pipeline">4.2. Creating the Pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">4.2.1. Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-for-text-generation">4.3. Parameters for Text Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#max-new-tokens">4.3.1. <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#return-full-text">4.3.2. <code class="docutils literal notranslate"><span class="pre">return_full_text</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature">4.3.3. <code class="docutils literal notranslate"><span class="pre">temperature</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-sample">4.3.4. <code class="docutils literal notranslate"><span class="pre">do_sample</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-the-output-portuguese">4.4. Generating the Output (portuguese)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#templates-and-prompt-engineering">4.5. Templates and Prompt Engineering</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lucas A. Souza
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>